<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background: url('./images/bg.jpg')
    repeat fixed;
    background-size:cover;
    overflow:scroll;
    padding: 100px;
    width: 1400px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Raleway', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Raleway', sans-serif;
  }
  .results {
  border: 1px solid black;
  border-collapse: collapse;
}
table, tr, td {
  padding: 12px;
}
</style>
<title>CS 184 Final Project Proposal</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Raleway:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>


<body>

<h1 align="middle" style="color:white;">CS 184: Computer Graphics and Imaging, Spring 2021</h1>
<h1 align="middle" style="color:white;">Final Project Proposal: Music-reactive Real-time Fire Simulation</h1>
<h2 align="middle" style="color:white;">Henry Allen, Priyanka Kargupta, Hannah Ku, Amber Xie</h2>

<br><br>

<div style="background-color: #ffffffeb; padding:40px; border-radius: 25px;">

<h2 align="middle">Summary</h2>
<p>In this project, we will create an audio visualizer using fire simulations that react to changes in features in music like frequency, tempo, and key. We will represent a set of frequency bands as a set of fires, and each fire will change in color, amplitude, or smokiness in real time according to the aforementioned music feature changes. Our final product will be a video that shows the fire simulations corresponding to a few different songs.
</p>


<h2 align="middle">Problem Description:</h2>
<p>Fire simulations on their own are very dynamic and visually exciting, but we wanted to find a way to showcase how fires react in real time to different physical changes. We realized that using the dynamic properties of music could be a great way to modulate these physical changes in a fire simulation, as music pieces can range from slow and somber, like a dying fire, to loud and high-tempo, like a fire with gasoline poured on it. By tying the size and other physical parameters of our fire simulation to music, we can show the full range of “emotion” of our fires.
  <br><br>
This project will give us important information on how fire simulations perform in a challenging, real-time environment. The hardware we will be using isn’t particularly high end, unlike the hardware used for simulation research at Nvidia or a Berkeley lab, so the code we create could have some unique optimizations for laptops and mid-grade GPUs. Further, our final demo will visualize fire simulations and time-frequency analysis in an exciting and instructional way.
<br><br>
The major challenge in this project is the real time rendering of multiple fires. Rendering a single fire is fairly complex, and it will take some optimization to get multiple fires running at the same time and have them modulate parameters according to the music. It will also be hard to have the fires sync up to the music properly, as we will have to render the fires at the same speed as the music plays, and we will have to make sure physical changes to the fire are quick enough to match the changes in the music.
<br><br>
We will first implement a single fire simulation that we can modulate with some parameters. Once this is running, we will implement a time-frequency analysis of a music sample to get parameters to modulate our fire. Finally, we will create a line of our controllable fires, each fire corresponding to a different frequency band. As a final demo, we will see how our fire simulation reacts to a few snippets of songs that we input. Our project will be coded in OpenGL and C++.
</p>

<h2 align="middle">Goals & Deliverables:</h2>

<h3 align="middle">Planned Delivery:</h3>

<p>
  We will implement a music visualizer composed of a row of fire flames, which will increase or decrease in height (following a wave-like form). The fires will react to a given music input, changing in color and amplitude based on Fourier analysis of an audio clip and its valence values. We may need to create segments of flames in order to independently customize their amplitude and color according to the audio analysis.In order to measure the quality and performance of our system, we will be analyzing our program’s FPS values. We will also compare our system’s output waveforms for a particular song to waveforms produced by other online audio visualizers. Some questions we would like to answer are (1) how is fire realistically rendered? (2) how can we manipulate fire to react to outside variables (e.g. audio)? (3) How does our fire simulation react to changes in music volume, tempo, and key? (4) How do we render our fire simulation efficiently?
</p>

<h3 align="middle">Aspirational Delivery:</h3>

<p>In the case that we have additional time and are ahead of schedule, we aspire to enhance the user experience. Specifically, we can add more user features such as toggling between different color palettes or parameters such as audio sensitivity, which could affect the amplitude and color range based on the fire’s sensitivity to the audio clip. Additionally, we hope to add audio customization where the users will be able to input their own selected audio clip and integrate the Spotify API in order to perform a more in depth analysis of the audio, which can be utilized to change the color, height, and smokiness of the fire. Some of the interesting features returned from the Tracks API include danceability, key, energy, and liveliness. An example of how this could be used is a cool-toned color palette and increased smokiness associated with a song with low energy and a minor key. Finally, we could render a disco ball at the top of the frame to enhance the listening experience.</p>
<br>

<center><img src="./images/mockup.png" align="middle" width="800px"/></center>
<br>
<br>
<h2 align="middle">Schedule:</h2>
<h3>Week 1 (April 12-16)</h3>
<ul>
  <li>Render basic fire using OpenGL (using resources we have found)</li>
</ul>

<h3>Week 2 (April 19-23)</h3>
<ul>
  <li>Finish rendering basic fire</li>
  <li> Audio analysis with mp3</li>
  <ul>
    <li>Implement Fourier analysis calculations</li>
  </ul>
  <li>Film milestone status report (due April 27th)</li>
</ul>
<h3>Week 3 (April 26-30)</h3>
<ul>
  <li>Incorporate fire with audio analysis</li>
  <ul>
    <li>Create a row of individual fires </li>
    <li>Change color and amplitude of flames based on our audio analysis</li>
  </ul>
</ul>
<h3>Week 4 (May 3-7)</h3>
<ul>
  <li>Connect to Spotify API / other user input</li>
  <li>Create video demo + final presentation</li>
</ul>
<br>
<h2 align="middle">Resources</h2>
<ul>
  <li>Software Resources</li>
  <ul>
    <li>C++</li>
    <li>OpenGL</li>
    <li>Music MP3s</li>
  </ul>
  <li>Hardware</li>
  <ul>
    <li>Laptops</li>
  </ul>
  <li>Links</li>
  <ul>
    <li><a href="http://ryansumner.github.io/FireParticles/">OpenGL Particles</a></li>
    <li><a href="https://cs184-firesim.github.io/final-report/"></a>Previous year's report</li>
    <li><a href="https://learnopengl.com/In-Practice/2D-Game/Particles">OpenGL Particles</a></li>
    <li><a href="https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-audio-analysis">Spotify API Audio Analysis</a></li>
    <li><a href="http://www.sci.utah.edu/~vpegorar/research/2006_EGWNP.pdf">Research</a></li>
    <li><a href="http://graphics.ucsd.edu/~henrik/papers/fire/fire.pdf">Physics-Based Fire</a></li>
    <li><a href="https://developer.nvidia.com/gpugems/gpugems3/part-v-physics-simulation/chapter-30-real-time-simulation-and-rendering-3d-fluids">3D Rendering</a></li>
  </ul>
</ul>
</body>
</html>
